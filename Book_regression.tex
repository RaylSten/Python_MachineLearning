%\input{../preamble_math}   
%\title{\shadowbox{淺層機器學習:迴歸學習器之探討\,}}
%\author{\small 陳柏維 \\ \small \it 國立臺北大學統計研究所}
%\date{\small \today}
%\begin{document}
%\maketitle
%\fontsize{12}{20pt}\selectfont
%\setlength{\parindent}{0pt}

\chapter{淺層機器學習:迴歸學習器之探討}
大數據的應用毫無疑問地是未來科技發展重要的一環，然而要發揮資料的價值就不能忽略機器學習與人工智慧。本文將介紹一般線性迴歸模型與加廣型迴歸模型，兩種監督式學習之學習器，最後透過生成模擬資料的訓練評比兩種學習器。

\section{淺層機器學習介紹}

機器學習(Machine Learning)依資料的特性概分兩種型態：監督式學習(Surpervised Learning)與非監督式學習(Unsupervised Learning)。監督式學習應用在成對的資料 $(x_i, y_i)^{N}_{i = 1}$，其中 $x_i, y_i$ 分別代表變數 $X$ 與 $Y$ 的樣本資料。監督學習的目的是透過這些已知的資料，確立變數 $X$ 與 $Y$ 之間的相關性，通常以數學式 $Y = f(X)$ 表示。所謂「監督式」與「非監督」的差別在因變數 $Y$ 是否已知。

\subsection{一般線性迴歸模型}

我們有一個輸入向量 $X^T = (X_1, X_2, \cdots, X_{p-1})$，並且想要預測一個實值輸出 $Y$，便可考慮使用一個配適有 $p-1$ 個預測變數 $X_1, \cdots, X_{p-1}$ 的迴歸模型進行分析，如式 (\ref{eq:regression_1})。

\begin{equation}\label{eq:regression_1}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_{p-1} X_{i, p-1} + \epsilon_i
\end{equation}

式(\ref{eq:regression_1}) 稱為「具 $p-1$ 個預測變數的第一階模型」，此模型也可以寫成 :
\[Y_i = \beta_0 + \sum_{k = 1}^{p-1}\beta_k X_{ik} + \epsilon_i\]

在將一般線性迴歸模型 (\ref{eq:regression_1}) 用矩陣的形式表示前，先定義以下矩陣:

$\mathop{\mathbf{Y}} \limits_{n \times 1} = \begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}$ \quad $\mathop{\mathbf{X}} \limits_{n \times p} = \begin{bmatrix}
1 & X_{11} & X_{12} & \cdots & X_{1, p-1}\\
1 & X_{21} & X_{22} & \cdots & X_{2, p-1}\\
\vdots  & \vdots & \vdots & & \vdots\\
1 & X_{n1} & X_{n2} & \cdots & X_{n, p-1}
\end{bmatrix}$ \quad $\mathop{\boldsymbol{\beta}} \limits_{p \times 1} = \begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots \\
\beta_{p-1}
\end{bmatrix}$ \quad $\mathop{\boldsymbol{\epsilon}} \limits_{n \times 1} = \begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots \\
\epsilon_n
\end{bmatrix}$

\bigskip
$\mathbf{Y}$ 與 $\boldsymbol{\epsilon}$ 的定義與簡單線性迴歸模型相同， $\boldsymbol{\beta}$ 向量新增了一些迴歸參數，$\mathbf{X}$ 矩陣包含第一行全為 $1$，另外有 $p-1$ 行分別代表 $p-1$ 個預測變數，在 $\mathbf{X}$ 矩陣中的任一元素 $X_{ik}$，其列下標 $i$ 表示第 $i$ 次試驗或是觀察個案，行下標 $k$ 表示第 $k$ 個預測變數。

定義完上述之矩陣後，接下來用矩陣的形式來表示一般線性迴歸模型 (\ref{eq:regression_1}) 如下 :
\[\mathop{\mathbf{Y}} \limits_{n \times 1} = \mathop{\mathbf{X}} \limits_{n \times p}\,\mathop{\boldsymbol{\beta}} \limits_{p \times 1} + \mathop{\boldsymbol{\epsilon}} \limits_{n \times 1}\]

\subsubsection{迴歸係數的估計}

首先從簡單線性迴歸模型出發，為了找到有關迴歸參數 $\beta_0$ 與 $\beta_1$ 所謂「好的」估計量\footnote{由 Gauss-Markov 定理可知，透過最小平方法得到之估計量 $\hat{\beta_0}$ 與 $\hat{\beta_1}$ 為一組不偏估計量，且在所有部篇估計量中，該組估計量之變異數為最小。}，我們透過\textbf{最小平方法}(method of least squares)，對於每一個觀測值 $(X_i, Y_i)$，最小平方法考慮了 $Y_i$ 與本身期望值之離差 :
\[Y_i - (\beta_0 + \beta_1 X_i)\]
而最小平方法便是將上述 $n$ 個離差平方後取總和，我們用符號 $Q$ 表示 :

\begin{equation}\label{eq:regression_2}
Q = \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 X_i)^2
\end{equation}

根據最小平方法之原理，$\beta_0$ 與 $\beta_1$ 的估計量為 $\hat{\beta_0}$ 與 $\hat{\beta_1}$，它們是能夠讓所有給定的樣本觀測值 $(X_1, Y_1), (X_2, Y_2), \cdots ,(X_n, Y_n)$ 計算出之 $Q$ 為最小之數。

將式(\ref{eq:regression_2})中的最小平方準則推廣至一般線性迴歸模型(\ref{eq:regression_1})中，成為 :

\[Q = \sum_{i = 1}^{n} (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i, p-1})^2\]

最小平方估計量就是在滿足能使 $Q$ 最小化的 $\beta_0, \beta_1, \cdots, \beta_{p-1}$，用向量 $\mathop{\boldsymbol{\hat{\beta}}} \limits_{p \times 1}$ 表示所得到的最小平方法估計量 $\hat{\beta}_0, \hat{\beta}_1, \cdots, \hat{\beta}_{p-1}$ :

\[\mathop{\boldsymbol{\hat{\beta}}} \limits_{p \times 1} = \begin{bmatrix}
\hat{\beta}_0\\
\hat{\beta}_1\\
\vdots \\
\hat{\beta}_{p-1}
\end{bmatrix}\]

一般線性迴歸模型(\ref{eq:regression_1})的最小平方標準方程式為 :
\[\mathbf{X}^T \mathbf{X} \boldsymbol{\hat{\beta}} = \mathbf{X}^T \mathbf{y}\]

而最小平方估計量 $\boldsymbol{\hat{\beta}}$ 為 :
\begin{equation}\label{eq:regression_3}
\boldsymbol{\hat{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\end{equation}
\subsubsection{一般線性迴歸模型之訓練}
在此訓練中，假設輸入向量為 $X^T = (X_1, X_2)$，且輸出值 $y$ 根據其類別，非 $0$ 即 $1$，因此將類別資料量化之後的問題，便可以直接套入以下的線性迴歸模式(Linear Regression Model)來分析，
\begin{equation}\label{eq:regression_4}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2}
\end{equation}
而迴歸係數 $\beta_0, \beta_1, \beta_2$ 由最小平方法求得的最佳解為式 (\ref{eq:regression_3})，當迴歸模型的參數 $\boldsymbol{\hat{\beta}}$ 估計完成，我們說機器完成了學習。接下來便可以拿來對新資料做群組屬性的判別(預測)。規則如 :

\tcbset{colback=blue!10!white}
\begin{tcolorbox}[title = {群組判別}]
當給予一個新的輸入資料 $x = (x_1, x_2)$，根據迴歸模型 (\ref{eq:regression_4})，
其輸出擬合值為：
\begin{equation}\label{eq:regression_5}
\hat{y} = \mathbf{x}^T \hat{\beta}
\end{equation}
其中 $x^T = [1 \,\,x_1 \,\,x_2]$。在迴歸模型下的擬合值 $\hat{y}$ 不一定剛好是 $0$ 或 $1$，它
可以是任何數值，但作為類別判斷時，可以依下列規則判別：假設 $G$ 代表判定的類別：
\[G = \begin{cases} 
\text{Group A}  & \mbox{if}\,\,\,\hat{y} \leq 0.5 \\ 
\text{Group B}  & \mbox{if}\,\,\,\hat{y} \geq 0.5
\end{cases}\]
\end{tcolorbox}
上述規則以 $\hat{y} = \mathbf{x}^T \hat{\beta} = 0.5$ 做為平面空間中兩個群組的分界線，將 $\mathbb{R}^2$ 平面一分為二，線的一邊以集合 $\{\mathbf{x}\,|\,\mathbf{x}^T \hat{\beta} \leq 0.5\}$ 代表 Group A，另一邊則為 Group B。

\newpage
下列將透過上述群組判別之規則進行訓練，其訓練之三組資料為 \verb|la_1.txt|, \verb|la_2.txt|, \verb|la_3.txt|。\footnote{資料來源 : https://ntpuccw.blog/python-in-learning/}

\textbf{\large 訓練 1.} 

在不使用 Python 套件下，根據輸出資料 \verb|Y| 的類別，在 \verb|X1-X2| 平面上以不同顏色描繪出群組的散佈圖，利用估計出的迴歸模型參數(\ref{eq:regression_3})畫出式(\ref{eq:regression_5})中 $\hat{y} = 0.5$ 的分界線，最後透過配適時的資料進行預測，並利用擬合值與原始值的誤差，計算訓練資料的準確率。在訓練 1. 中，所使用之訓練資料為資料檔 \verb|la_1.txt|，其內容如圖 \ref{fig:data_1} 所示。

\begin{figure}[H]
    \centering
        \includegraphics[scale=0.6]{\imgdir data_1.jpg}
    \caption{la\_1.txt 資料檔的內容}
    \label{fig:data_1}
\end{figure}

利用 Python 讀取 \verb|la_1.txt| 資料檔，並且將資料檔中的前兩行定義為 \verb|x|，第三行定義為 \verb|y|。由於資料檔為 \verb|txt| 的文字檔型態，且圖 \ref{fig:data_1} 中的第一行以 \,\% 作為註解行，因此在輸入資料的指令中必須明示註解行的標示 \verb|np.loadtxt(..., comments = '%')|，以便在讀取資料時排除 :
\bigskip
\begin{lstlisting}[language = Python]
data_dir = 'data/'
D = np.loadtxt(data_dir + 'la_1.txt', comments = '%')
x = D[:, 0:2]
y = D[:, 2]
\end{lstlisting}

在畫散佈圖前利用矩陣索引的方式，擷取矩陣中所需的部分資料，將資料依照群組分開，接著利用 \verb|plot| 指令繪製散佈圖，並加上標籤(label)文字，其程式碼如下:
\bigskip
\begin{lstlisting}[language = Python]
idx_0 = (D[:, 2] == 0) # y = 0
plt.plot(D[idx_0, 0], D[idx_0, 1], 'go', markersize = 5, \
	markerfacecolor = 'none', label = 'Group A')
idx_1 = (D[:, 2] == 1) # y = 1
plt.plot(D[idx_1, 0], D[idx_1, 1], 'co', markersize = 5, \
	markerfacecolor = 'none', label = 'Group B')
plt.xlabel('x1'), plt.ylabel('x2')
plt.legend()
\end{lstlisting}

要計算式(\ref{eq:regression_3}) 前，需先建構資料矩陣 $X$ 與 $\mathbf{y}$，接著代入(\ref{eq:regression_3})並繪製分界線，其程式碼如下 :
\bigskip
\begin{lstlisting}[language = Python]
n = len(D[:, 0])
X = np.hstack((np.ones((n, 1)), D[:, 0:2]))  
b_hat = LA.inv(X.T @ X) @ X.T @ y.T
x_1 = np.array([1.5, 4.5])
y_1 = (0.5 - b_hat[1] * x_1 - b_hat[0]) / b_hat[2]
plt.plot(x_1, y_1, c = 'purple', lw = 2)
\end{lstlisting}

最後透過配適時的資料進行預測，並利用擬合值與原始值的誤差，計算訓練資料的準確率，其呈現結果如下列程式碼 :
\bigskip
\begin{lstlisting}[language = Python]
y_hat = X @ b_hat
y_pre = [1 if i > 0.5 else 0 for i in y_hat]
Acc = 100 * np.mean(y_pre == y)
\end{lstlisting}

圖 \ref{fig:la_1} 為資料檔 \verb|la_1.txt| 透過估計出的一般線性迴歸模型參數(\ref{eq:regression_3})畫出式(\ref{eq:regression_5})中 $\hat{y} = 0.5$ 的分界線之成果圖，其訓練之準確率為 $94.00	\%$。
\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir la_1.eps}
    \caption{一般線性迴歸模型的分界線 : 資料檔 la\_1.txt}
    \label{fig:la_1}
\end{figure}

\textbf{\large 訓練 2.}

透過 Python 的 \verb|sklearn| 套件當中的 \verb|linear_model| 模組，其指令為 \verb|LinearRegression| 建立一般線性迴歸模型(\ref{eq:regression_4})，此訓練也將透過資料檔 \verb|la_2.txt| 展示套件之使用方式、繪製分界線及計算訓練之準確率，如圖 \ref{fig:la_2} 所示。

首先載入 \verb|sklearn| 套件，透過 \verb|LinearRegression()|建立線性迴歸模型，接著利用 \verb|.fit()| 進行資料配適，最後再利用 \verb|.intercept_| 與 \verb|.coef_| 呈現迴歸參數估計之結果，程式碼如下 :
\bigskip
\begin{lstlisting}[language = Python]
from sklearn.linear_model import LinearRegression

Mdl = LinearRegression()# 建立線性迴歸模型
Mdl.fit(X, y) # 資料配適
intrcp = Mdl.intercept_ # beta_0
coeffs = Mdl.coef_ # beta_1, beta_2
\end{lstlisting}

透過資料配適之迴歸參數，畫出式(\ref{eq:regression_5})中 $\hat{y} = 0.5$ 的分界線，程式碼如下 :
\bigskip
\begin{lstlisting}[language = Python]
x = np.array([1.5, 4.5])
f = -(intrcp - 0.5 + coeffs[0] * x) / coeffs[1]
plt.plot(x, f, c = 'purple', lw = 2)
\end{lstlisting}

最後計算擬合值並進行群組判讀，計算訓練之準確率，程式碼如下 :
\bigskip
\begin{lstlisting}[language = Python]
y_hat = Mdl.predict(X) # 計算擬合值
y_pre = [1 if i > 0.5 else 0 for i in y_hat] # 群組判讀
Acc = 100 * np.mean(y_pre == y) # 計算準確率
\end{lstlisting}

圖 \ref{fig:la_2} 為資料檔 \verb|la_2.txt| 透過估計出的一般線性迴歸模型參數(\ref{eq:regression_3})畫出式(\ref{eq:regression_5})中 $\hat{y} = 0.5$ 的分界線之成果圖，其訓練之準確率為 $91.00	\%$。
\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir la_2.eps}
    \caption{一般線性迴歸模型的分界線 : 資料檔 la\_2.txt}
    \label{fig:la_2}
\end{figure}

\textbf{\large 訓練 3.}

此訓練將透過資料檔 \verb|la_3.txt| 展示在使用與不使用套件下繪製分界線及計算訓練之準確率之對比，如圖 \ref{fig:la_3} 所示，其中圖 \ref{fig:la_3} 左邊透過 \verb|sklearn| 套件呈現，圖 \ref{fig:la_3} 右邊則為不使用套件之呈現，兩者透過估計出的一般線性迴歸模型參數(\ref{eq:regression_3})畫出式(\ref{eq:regression_5})中 $\hat{y} = 0.5$ 的分界線，其訓練之準確率皆為 $73.00\%$。
\bigskip
\begin{figure}[H]
\centering
\includegraphics[scale = 0.483]{\imgdir la_3_2.eps}
\includegraphics[scale = 0.483]{\imgdir la_3_1.eps}
\caption{一般線性迴歸模型的分界線 : 資料檔 la\_3.txt} 
\label{fig:la_3}
\end{figure}

\subsection{加廣型迴歸模型(Augmented Regression Model)}
假設輸入變數為 $X_1, X_2$，則 $(X_1, X_2)$ 所有可能的值涵蓋二度空間。此時如果將
兩個變數擴展為五個變數 $X1, X_2, X_1X_2, X_1^2, X_2^2$，同樣利用迴歸模式與最小平方
法建立一條分界線，當將此分界線投映回原來的空間時，它將呈現出一條曲線。這五個變數因其彼此相關的本質，並非將空間拓展為五度空間，實際仍在二度空間裡，這個所謂的加廣型迴歸模型寫成
\begin{equation}\label{eq:regression_6}
Y = \beta_0 + \beta_1X_{1} + \beta_2X_{2} + \beta_3 X_{1}X_{2} + \beta_4 X_{1}^2 + \beta_5 X_{2}^2
\end{equation}
接著計算參數 $\hat{\beta}$ 的最小平方估計 ： $\hat{\beta} = (X^T X)^{-1} X^T y$，如同線性迴歸模式，則式(\ref{eq:regression_6})的加廣型迴歸模型的分界線表示為集合
\[\big\{(X_1, X_2)\,|\,\hat{\beta}_0 + \hat{\beta}_1X_1 + \hat{\beta}_2X_2 + \hat{\beta}_3X_1X_2 + \hat{\beta}_4X_1^2 + \hat{\beta}_5X_2^2 = 0.5\big\}\]
因此，我們可以將函數寫成
\begin{equation}\label{eq:regression_7}
f(X_1, X_2) = \hat{\beta}_0 - 0.5 + \hat{\beta}_1X_1 + \hat{\beta}_2X_2 + \hat{\beta}_3X_1X_2 + \hat{\beta}_4X_1^2 + \hat{\beta}_5X_2^2
\end{equation}
接著透過 Python 計算上述分界線的係數 $\hat{\beta}$，並利用等高線圖畫出分界線函數(\ref{eq:regression_7})，程式碼如下 ：
\bigskip
\begin{lstlisting}[language = Python]
x1 = D[:, 0:1] # n x 1 vector
x2 = D[:, 1:2]
X = np.hstack((np.ones((n, 1)), \
x1, x2, x1 * x2, x1 ** 2, x2 ** 2))
y = D[:, 2:3]
b = LA.pinv(X) @ y # pseudo inverse
f = (
    lambda x: b[0]
    + b[1] * x[0]
    + b[2] * x[1]
    + b[3] * x[0] * x[1]
    + b[4] * x[0] ** 2
    + b[5] * x[1] ** 2)

x_min, x_max = plt.xlim()
y_min, y_max = plt.ylim()
nx, ny = 100, 100
xx = np.linspace(x_min, x_max, nx)
yy = np.linspace(y_min, y_max, ny)
X, Y = np.meshgrid(xx, yy)
Z = f([X, Y])
contours = plt.contour(X, Y, Z, levels = [0.5], \
	colors = 'purple')
\end{lstlisting}

\textbf{\large 訓練 1.} 

此訓練將透過資料檔 \verb|la_1.txt| 繪製分界線及計算訓練之準確率，如圖 \ref{fig:aug_la_1} 所示，而與圖 \ref{fig:la_1} 對比可知，一般線性迴歸模型的分界線與加廣型迴歸模型的分界線相同，且其準確率皆為 $94\%$。

\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir aug_la_1.eps}
    \caption{加廣型迴歸模型的分界線 : 資料檔 la\_1.txt}
    \label{fig:aug_la_1}
\end{figure}

\textbf{\large 訓練 2.} 

此訓練將透過資料檔 \verb|la_2.txt| 繪製分界線及計算訓練之準確率，如圖 \ref{fig:aug_la_2} 所示，與圖 \ref{fig:la_2} 對比可看出加廣型迴歸模型的分界線能將資料切割得較好，而其準確率為 $93.5\%$ 也比一般線性迴歸模型的準確率 $91\%$ 高。

\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir aug_la_2.eps}
    \caption{加廣型迴歸模型的分界線 : 資料檔 la\_2.txt}
    \label{fig:aug_la_2}
\end{figure}

\textbf{\large 訓練 3.} 

此訓練將透過資料檔 \verb|la_3.txt| 繪製分界線及計算訓練之準確率，如圖 \ref{fig:aug_la_3} 所示，與圖 \ref{fig:la_3} 對比後，發現加廣型迴歸模式的分界線的表現不一定比線性迴歸模式好，而其準確率為 $72.5\%$ 也確實比一般線性迴歸模型的準確率 $73\%$ 低。

\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir aug_la_3.eps}
    \caption{加廣型迴歸模型的分界線 : 資料檔 la\_3.txt}
    \label{fig:aug_la_3}
\end{figure}
\subsection{學習器之評比}
在機器學習的領域，將不同的學習方法(模型)通稱為學習器，例如一般線性迴歸模型與加廣型迴歸模型都是學習器。而學習器的選擇、訓練與評比是機器學習的重要步驟。前兩節選擇兩種學習器並經過實務資料的訓練，接著將透過模擬雙變量常態母體的資料，探討學習器面對不同的資料時的表現。

\textbf{\large 模擬訓練 1.(資料中心位置較遠且分散程度小)}

假設資料來自兩個雙變量常態的母體，其平均數、共變異矩陣與樣本數分別為 ：

\[\mu_1 = \begin{bmatrix}
0 \\
0
\end{bmatrix}, \quad \mu_2 = \begin{bmatrix}
3 \\
3
\end{bmatrix}, \quad \begin{matrix} \sum_{1} \end{matrix} = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}, \quad \begin{matrix} \sum_{2} \end{matrix} = \begin{bmatrix}
1   & 0\\
0 & 1
\end{bmatrix}\]

\[n_1 = 200, \quad n_2 = 200\]

將上述資料以 Python 的程式碼呈現，如下 :
\bigskip
\begin{lstlisting}[language = Python]
n1, n2 = 200, 200
m1, m2 = np.array([0, 0]), np.array([3, 3])
Cov1 = np.array([[1, 0], [0, 1]])     # 共變異數矩陣
Cov2 = np.array([[1, 0], [0, 1]])
\end{lstlisting}

接著隨機產生兩個母體的資料，並將資料垂直堆疊，程式碼如下 : 
\bigskip
\begin{lstlisting}[language = Python]
mvn1 = multivariate_normal(mean = m1, cov = Cov1)
mvn2 = multivariate_normal(mean = m2, cov = Cov2)
A, B = mvn1.rvs(n1), mvn2.rvs(n2)
X = np.vstack((A, B))  # 將 A, B 垂直堆疊
y = np.hstack((np.zeros(n1), np.ones(n2))) # 水平堆疊
\end{lstlisting}

利用 \verb|np.savetxt()| 將資料存成 \verb|txt| 檔，呈現如下列程式碼與圖 \ref{fig:test_data_1} : 
\bigskip
\begin{lstlisting}[language = Python]
np.savetxt('test_1.txt', np.c_[X, y],\
	fmt = '%.4f %.4f %d', header = 'X1 X2 y')
\end{lstlisting}

\begin{figure}[H]
    \centering
        \includegraphics[scale=0.8]{\imgdir test_data_1.jpg}
    \caption{生成模擬訓練資料 1. 的內容}
    \label{fig:test_data_1}
\end{figure}

由圖 \ref{fig:test_1} 可知，在資料中心位置較遠的情況下，兩種迴歸模型將資料切割的情形都不錯，而一般線性迴歸模型的訓練誤差低於加廣型迴歸模型。
\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir test_1.eps}
    \caption{兩個迴歸模型的分群錯誤率比較:模擬訓練 1}
    \label{fig:test_1}
\end{figure}

\textbf{\large 模擬訓練 2.(資料中心位置較遠且分散程度大)}

假設資料來自兩個雙變量常態的母體，其平均數、共變異矩陣與樣本數分別為 ：

\[\mu_1 = \begin{bmatrix}
0 \\
0
\end{bmatrix}, \quad \mu_2 = \begin{bmatrix}
3 \\
3
\end{bmatrix}, \quad \begin{matrix} \sum_{1} \end{matrix} = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}, \quad \begin{matrix} \sum_{2} \end{matrix} = \begin{bmatrix}
5 & 0\\
0 & 5
\end{bmatrix}\]

\[n_1 = 200, \quad n_2 = 200\]

由圖 \ref{fig:test_2} 可知，在資料中心位置較遠且分散程度大的情況下，兩種迴歸模型將資料切割的情形都還算不錯，而一般線性迴歸模型的訓練誤差低於加廣型線性迴歸模型。
\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir test_2.eps}
    \caption{兩個迴歸模型的分群錯誤率比較:模擬訓練 2}
    \label{fig:test_2}
\end{figure}

\textbf{\large 模擬訓練 3.(資料中心位置接近且分散程度小)}

假設資料來自兩個雙變量常態的母體，其平均數、共變異矩陣與樣本數分別為 ：

\[\mu_1 = \begin{bmatrix}
0 \\
0
\end{bmatrix}, \quad \mu_2 = \begin{bmatrix}
1 \\
1
\end{bmatrix}, \quad \begin{matrix} \sum_{1} \end{matrix} = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}, \quad \begin{matrix} \sum_{2} \end{matrix} = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}\]

\[n_1 = 200, \quad n_2 = 200\]

由圖 \ref{fig:test_3} 可知，在資料中心位置接近且分散程度小的情況下，兩種迴歸模型將資料切割的情形都不是很好，而加廣型迴歸模型的訓練誤差低於一般線性迴歸模型，造成訓練誤差都蠻高的原因可能為資料過於接近，導致無法分割得較乾淨。
\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir test_3.eps}
    \caption{兩個迴歸模型的分群錯誤率比較:模擬訓練 3}
    \label{fig:test_3}
\end{figure}

\textbf{\large 模擬訓練 4.(資料中心位置接近且分散程度大)}

假設資料來自兩個雙變量常態的母體，其平均數、共變異矩陣與樣本數分別為 ：

\[\mu_1 = \begin{bmatrix}
0 \\
0
\end{bmatrix}, \quad \mu_2 = \begin{bmatrix}
1 \\
1
\end{bmatrix}, \quad \begin{matrix} \sum_{1} \end{matrix} = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}, \quad \begin{matrix} \sum_{2} \end{matrix} = \begin{bmatrix}
5 & 0\\
0 & 5
\end{bmatrix}\]

\[n_1 = 200, \quad n_2 = 200\]

由圖 \ref{fig:test_4} 可知，在資料中心位置接近且分散程度大的情況下，兩種迴歸模型將資料切割的情形都不是很好，加廣型迴歸模型的訓練誤差低於一般線性迴歸模型，而有趣的是加廣型迴歸模型的分界線為一個橢圓。
\begin{figure}[H]
    \centering
        \includegraphics[scale=0.7]{\imgdir test_4.eps}
    \caption{兩個迴歸模型的分群錯誤率比較:模擬訓練 4}
    \label{fig:test_4}
\end{figure}

將四個模擬訓練的訓練誤差整理成表 \ref{tb:mis_1}，由表 \ref{tb:mis_1} 可知，兩個學習器在資料較為接近的情況下，訓練誤差皆會高於資料相距較遠的情況，而除了當資料中心位置較遠且分散程度小的情況之外，加廣型迴歸模型的訓練誤差皆低於一般線性迴歸模型。對於這個結果其實我一點也不意外，畢竟一般線性迴歸模型只用一條直線當作分界線，而加廣型迴歸模型是利用一條非線性的曲線作為分界線。

\begin{table}[H] 
\centering
\caption{學習器訓練誤差之對比}\label{tb:mis_1}
\tabcolsep=12pt
\begin{tabular}{ccc} 
\toprule
& \multicolumn{2}{c}{學習器}\\
\cmidrule(l){2-3}
資料型態 & 一般線性迴歸模型 & 加廣型迴歸模型 \\[3pt]
\midrule
資料中心位置較遠且分散程度小 & \cellcolor{red!25}0.0200 & 0.0250 \\[3pt]
資料中心位置較遠且分散程度大 & 0.1100 & \cellcolor{red!25}0.0975 \\[3pt]
資料中心位置接近且分散程度小 & 0.2500 & \cellcolor{red!25}0.2425 \\[3pt]
資料中心位置接近且分散程度大 & 0.3200 & \cellcolor{red!25}0.2525 \\ 
\bottomrule
\end{tabular}
\end{table}

\section{證明題}
證明 $\hat{\beta} = (X^T X)^{-1}X^T \mathbf{y}$ 是迴歸模型 $Y = \beta + \beta X_1 + \beta X_2$ 的最小平方法解，即 \[\hat{\beta} \doteq arg \mathop{\min}\limits_{\beta} ||X \beta - \mathbf{y}||^2\]

\rule{15cm}{0.5mm}

Let $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - X \hat{\beta}$.

$\because \mathbf{e^T e} = \begin{bmatrix}
e_1 & e_2  & \cdots & e_n
\end{bmatrix}_{1 \times n} \begin{bmatrix}
e_1    \\
e_2    \\
\vdots \\
e_n      
\end{bmatrix}_{n \times 1} = \begin{bmatrix}
e_1^2 + e_2^2 + \cdots + e_n^2
\end{bmatrix}_{1 \times 1}$ $\displaystyle = \sum_{i = 1}^n e_i^2 = $ SSE

$\Rightarrow$ SSE $\displaystyle = \sum_{i = 1}^n e_i^2 = \mathbf{e^T e}$

\quad \quad \quad \,\,$= (\mathbf{y} - X \hat{\beta})^T (\mathbf{y} - X \hat{\beta})$

\quad \quad \quad \,\,$= (\mathbf{y^T} - \hat{\beta}^T X^T) (\mathbf{y} - X \hat{\beta})$

\quad \quad \quad \,\,$= \mathbf{y^T}\mathbf{y} - \mathbf{y^T}X\hat{\beta} - \hat{\beta}^T X^T \mathbf{y} + \hat{\beta}^T X^T X \hat{\beta}$

\quad \quad \quad \,\,$= \mathbf{y^T}\mathbf{y} - 2\hat{\beta}^T X^T \mathbf{y} + \hat{\beta}^T X^T X \hat{\beta}$

$\displaystyle \Rightarrow \frac{\partial \text{SSE}}{\partial \hat{\beta}} = -2 X^T \mathbf{y} + 2 X^T X \hat{\beta}$

Let $\displaystyle \frac{\partial \text{SSE}}{\partial \hat{\beta}} = \mathbf{0}$.

$\Rightarrow -2 X^T \mathbf{y} + 2 X^T X \hat{\beta} = \mathbf{0}$

$\Rightarrow (X^T X)\hat{\beta} = X^T \mathbf{y}$

If the inverse of $(X^T X)$ exists.

$\Rightarrow (X^T X)^{-1}(X^T X)\hat{\beta} = (X^T X)^{-1}X^T \mathbf{y}$

$\Rightarrow I\hat{\beta} = (X^T X)^{-1}X^T \mathbf{y}$, where $I$ is a $3 \times 3$  identity matrix

$\Rightarrow \hat{\beta} = (X^T X)^{-1}X^T \mathbf{y}$

Hence the proof is completed.

\section{結語}
本次的作品正式進入了淺層機器學習的領域，介紹了兩種監督式學習之學習器，分別為一般線性迴歸模型
和加廣型迴歸模型，並且說明了兩種學習器之理論、分群方法與實作，最後透過生成模擬資料的訓練評比兩種學習器。
%\end{document}